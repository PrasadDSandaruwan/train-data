[
  {
    "id": 1087,
    "file_path": "src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveInputStream.java",
    "start-bug-line": 501,
    "end-bug-line": 501,
    "bug": "currEntry.setGroupId(Integer.parseInt(val));",
    "fix": "currEntry.setGroupId(Long.parseLong(val));",
    "fixes": [],
    "err": "",
    "ctxs": [
      {
        "txt": "package org.apache.commons.compress.archivers.tar; import java.io.ByteArrayOutputStream; import java.io.IOException;"
      },
      {
        "txt": "import java.io.InputStream; import java.util.HashMap; import java.util.Map; import java.util.Map.Entry; import org.apache.commons.compress.archivers.ArchiveEntry; import org.apache.commons.compress.archivers.ArchiveInputStream; import org.apache.commons.compress.archivers.zip.ZipEncoding; import org.apache.commons.compress.archivers.zip.ZipEncodingHelper; import org.apache.commons.compress.utils.ArchiveUtils; import org.apache.commons.compress.utils.CharsetNames;"
      },
      {
        "txt": "import org.apache.commons.compress.utils.IOUtils; public class TarArchiveInputStream extends ArchiveInputStream { private static final int SMALL_BUFFER_SIZE = 256; private final byte[] SMALL_BUF = new byte[SMALL_BUFFER_SIZE]; private final int recordSize; private final int blockSize; private boolean hasHitEOF; private long entrySize; private long entryOffset; private final InputStream is;"
      },
      {
        "txt": "private TarArchiveEntry currEntry; private final ZipEncoding zipEncoding; final String encoding; public TarArchiveInputStream(InputStream is) { this(is, TarConstants.DEFAULT_BLKSIZE, TarConstants.DEFAULT_RCDSIZE); } public TarArchiveInputStream(InputStream is, String encoding) { this(is, TarConstants.DEFAULT_BLKSIZE, TarConstants.DEFAULT_RCDSIZE, encoding); }"
      },
      {
        "txt": "public TarArchiveInputStream(InputStream is, int blockSize) { this(is, blockSize, TarConstants.DEFAULT_RCDSIZE); } public TarArchiveInputStream(InputStream is, int blockSize, String encoding) { this(is, blockSize, TarConstants.DEFAULT_RCDSIZE, encoding); } public TarArchiveInputStream(InputStream is, int blockSize, int recordSize) { this(is, blockSize, recordSize, null); }"
      },
      {
        "txt": "public TarArchiveInputStream(InputStream is, int blockSize, int recordSize, String encoding) { this.is = is; this.hasHitEOF = false; this.encoding = encoding; this.zipEncoding = ZipEncodingHelper.getZipEncoding(encoding); this.recordSize = recordSize; this.blockSize = blockSize; } @Override"
      },
      {
        "txt": "public void close() throws IOException { is.close(); } public int getRecordSize() { return recordSize; } @Override public int available() throws IOException { if (entrySize - entryOffset > Integer.MAX_VALUE) { return Integer.MAX_VALUE;"
      },
      {
        "txt": "} return (int) (entrySize - entryOffset); } @Override public long skip(final long n) throws IOException { if (n <= 0) { return 0; } final long available = entrySize - entryOffset; final long skipped = is.skip(Math.min(n, available));"
      },
      {
        "txt": "count(skipped); entryOffset += skipped; return skipped; } @Override public boolean markSupported() { return false; } @Override public void mark(int markLimit) {"
      },
      {
        "txt": "} @Override public synchronized void reset() { } public TarArchiveEntry getNextTarEntry() throws IOException { if (hasHitEOF) { return null; } if (currEntry != null) { IOUtils.skip(this, Long.MAX_VALUE);"
      },
      {
        "txt": "skipRecordPadding(); } byte[] headerBuf = getRecord(); if (headerBuf == null) { currEntry = null; return null; } try { currEntry = new TarArchiveEntry(headerBuf, zipEncoding); } catch (IllegalArgumentException e) {"
      },
      {
        "txt": "IOException ioe = new IOException(\"Error detected parsing the header\"); ioe.initCause(e); throw ioe; } entryOffset = 0; entrySize = currEntry.getSize(); if (currEntry.isGNULongLinkEntry()) { byte[] longLinkData = getLongNameData(); if (longLinkData == null) { return null;"
      },
      {
        "txt": "} currEntry.setLinkName(zipEncoding.decode(longLinkData)); } if (currEntry.isGNULongNameEntry()) { byte[] longNameData = getLongNameData(); if (longNameData == null) { return null; } currEntry.setName(zipEncoding.decode(longNameData)); }"
      },
      {
        "txt": "if (currEntry.isPaxHeader()){ // Process Pax headers paxHeaders(); } if (currEntry.isGNUSparse()){ // Process sparse files readGNUSparse(); } entrySize = currEntry.getSize(); return currEntry; } private void skipRecordPadding() throws IOException {"
      },
      {
        "txt": "if (this.entrySize > 0 && this.entrySize % this.recordSize != 0) { long numRecords = (this.entrySize / this.recordSize) + 1; long padding = (numRecords * this.recordSize) - this.entrySize; long skipped = IOUtils.skip(is, padding); count(skipped); } } protected byte[] getLongNameData() throws IOException { ByteArrayOutputStream longName = new ByteArrayOutputStream(); int length = 0;"
      },
      {
        "txt": "while ((length = read(SMALL_BUF)) >= 0) { longName.write(SMALL_BUF, 0, length); } getNextEntry(); if (currEntry == null) { return null; } byte[] longNameData = longName.toByteArray(); length = longNameData.length; while (length > 0 && longNameData[length - 1] == 0) {"
      },
      {
        "txt": "--length; } if (length != longNameData.length) { byte[] l = new byte[length]; System.arraycopy(longNameData, 0, l, 0, length); longNameData = l; } return longNameData; } private byte[] getRecord() throws IOException {"
      },
      {
        "txt": "byte[] headerBuf = readRecord(); hasHitEOF = isEOFRecord(headerBuf); if (hasHitEOF && headerBuf != null) { tryToConsumeSecondEOFRecord(); consumeRemainderOfLastBlock(); headerBuf = null; } return headerBuf; } protected boolean isEOFRecord(byte[] record) {"
      },
      {
        "txt": "return record == null || ArchiveUtils.isArrayZero(record, recordSize); } protected byte[] readRecord() throws IOException { byte[] record = new byte[recordSize]; int readNow = IOUtils.readFully(is, record); count(readNow); if (readNow != recordSize) { return null; } return record;"
      },
      {
        "txt": "} private void paxHeaders() throws IOException{ Map<String, String> headers = parsePaxHeaders(this); getNextEntry(); // Get the actual file entry applyPaxHeadersToCurrentEntry(headers); } Map<String, String> parsePaxHeaders(InputStream i) throws IOException { Map<String, String> headers = new HashMap<String, String>(); while(true){ // get length int ch;"
      },
      {
        "txt": "int len = 0; int read = 0; while((ch = i.read()) != -1) { read++; if (ch == ' '){ // End of length string ByteArrayOutputStream coll = new ByteArrayOutputStream(); while((ch = i.read()) != -1) { read++; if (ch == '='){ // end of keyword String keyword = coll.toString(CharsetNames.UTF_8);"
      },
      {
        "txt": "final int restLen = len - read; byte[] rest = new byte[restLen]; int got = IOUtils.readFully(i, rest); if (got != restLen) { throw new IOException(\"Failed to read \" + \"Paxheader. Expected \" + restLen + \" bytes, read \" + got); }"
      },
      {
        "txt": "String value = new String(rest, 0, restLen - 1, CharsetNames.UTF_8); headers.put(keyword, value); break; } coll.write((byte) ch); } break; // Processed single header } len *= 10;"
      },
      {
        "txt": "len += ch - '0'; } if (ch == -1){ // EOF break; } } return headers; } private void applyPaxHeadersToCurrentEntry(Map<String, String> headers) { for (Entry<String, String> ent : headers.entrySet()){"
      },
      {
        "txt": "String val = ent.getValue(); if (\"path\".equals(key)){ currEntry.setName(val); } else if (\"linkpath\".equals(key)){ currEntry.setLinkName(val); } else if (\"gid\".equals(key)){ <extra_id_0> } else if (\"gname\".equals(key)){ currEntry.setGroupName(val); } else if (\"uid\".equals(key)){ currEntry.setUserId(Integer.parseInt(val)); } else if (\"uname\".equals(key)){ currEntry.setUserName(val);"
      },
      {
        "txt": "} else if (\"uname\".equals(key)){ currEntry.setUserName(val); } else if (\"size\".equals(key)){ currEntry.setSize(Long.parseLong(val)); } else if (\"mtime\".equals(key)){ currEntry.setModTime((long) (Double.parseDouble(val) * 1000)); } else if (\"SCHILY.devminor\".equals(key)){ currEntry.setDevMinor(Integer.parseInt(val)); } else if (\"SCHILY.devmajor\".equals(key)){ currEntry.setDevMajor(Integer.parseInt(val));"
      },
      {
        "txt": "} } } private void readGNUSparse() throws IOException { sparses = new ArrayList(); sparses.addAll(currEntry.getSparses()); if (currEntry.isExtended()) { TarArchiveSparseEntry entry; do { byte[] headerBuf = getRecord();"
      },
      {
        "txt": "if (headerBuf == null) { currEntry = null; break; } entry = new TarArchiveSparseEntry(headerBuf); sparses.addAll(entry.getSparses()); } while (entry.isExtended()); } } @Override"
      },
      {
        "txt": "public ArchiveEntry getNextEntry() throws IOException { return getNextTarEntry(); } private void tryToConsumeSecondEOFRecord() throws IOException { boolean shouldReset = true; boolean marked = is.markSupported(); if (marked) { is.mark(recordSize); } try {"
      },
      {
        "txt": "shouldReset = !isEOFRecord(readRecord()); } finally { if (shouldReset && marked) { pushedBackBytes(recordSize); is.reset(); } } } @Override public int read(byte[] buf, int offset, int numToRead) throws IOException {"
      },
      {
        "txt": "int totalRead = 0; if (hasHitEOF || entryOffset >= entrySize) { return -1; } if (currEntry == null) { throw new IllegalStateException(\"No current tar entry\"); } numToRead = Math.min(numToRead, available()); totalRead = is.read(buf, offset, numToRead); if (totalRead == -1) {"
      },
      {
        "txt": "if (numToRead > 0) { throw new IOException(\"Truncated TAR archive\"); } hasHitEOF = true; } else { count(totalRead); entryOffset += totalRead; } return totalRead; }"
      },
      {
        "txt": "@Override public boolean canReadEntryData(ArchiveEntry ae) { if (ae instanceof TarArchiveEntry) { TarArchiveEntry te = (TarArchiveEntry) ae; return !te.isGNUSparse(); } return false; } public TarArchiveEntry getCurrentEntry() { return currEntry;"
      },
      {
        "txt": "} protected final void setCurrentEntry(TarArchiveEntry e) { currEntry = e; } protected final boolean isAtEOF() { return hasHitEOF; } protected final void setAtEOF(boolean b) { hasHitEOF = b; }"
      },
      {
        "txt": "private void consumeRemainderOfLastBlock() throws IOException { long bytesReadOfLastBlock = getBytesRead() % blockSize; if (bytesReadOfLastBlock > 0) { long skipped = IOUtils.skip(is, blockSize - bytesReadOfLastBlock); count(skipped); } } public static boolean matches(byte[] signature, int length) { if (length < TarConstants.VERSION_OFFSET+TarConstants.VERSIONLEN) { return false;"
      },
      {
        "txt": "} if (ArchiveUtils.matchAsciiBuffer(TarConstants.MAGIC_POSIX, signature, TarConstants.MAGIC_OFFSET, TarConstants.MAGICLEN) && ArchiveUtils.matchAsciiBuffer(TarConstants.VERSION_POSIX, signature, TarConstants.VERSION_OFFSET, TarConstants.VERSIONLEN) ){ return true; } if (ArchiveUtils.matchAsciiBuffer(TarConstants.MAGIC_GNU,"
      },
      {
        "txt": "signature, TarConstants.MAGIC_OFFSET, TarConstants.MAGICLEN) && ( ArchiveUtils.matchAsciiBuffer(TarConstants.VERSION_GNU_SPACE, signature, TarConstants.VERSION_OFFSET, TarConstants.VERSIONLEN) || ArchiveUtils.matchAsciiBuffer(TarConstants.VERSION_GNU_ZERO, signature, TarConstants.VERSION_OFFSET, TarConstants.VERSIONLEN) ) ){"
      },
      {
        "txt": "return true; } if (ArchiveUtils.matchAsciiBuffer(TarConstants.MAGIC_ANT, signature, TarConstants.MAGIC_OFFSET, TarConstants.MAGICLEN) && ArchiveUtils.matchAsciiBuffer(TarConstants.VERSION_ANT, signature, TarConstants.VERSION_OFFSET, TarConstants.VERSIONLEN) ){ return true; }"
      },
      {
        "txt": "return false; }"
      }
    ]
  },
  {
    "id": 1088,
    "file_path": "src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveInputStream.java",
    "start-bug-line": 505,
    "end-bug-line": 505,
    "bug": "currEntry.setUserId(Integer.parseInt(val));",
    "fix": "currEntry.setUserId(Long.parseLong(val));",
    "fixes": [],
    "err": "",
    "ctxs": [
      {
        "txt": "package org.apache.commons.compress.archivers.tar; import java.io.ByteArrayOutputStream; import java.io.IOException; import java.io.InputStream; import java.util.HashMap; import java.util.Map; import java.util.Map.Entry;"
      },
      {
        "txt": "import org.apache.commons.compress.archivers.ArchiveEntry; import org.apache.commons.compress.archivers.ArchiveInputStream; import org.apache.commons.compress.archivers.zip.ZipEncoding; import org.apache.commons.compress.archivers.zip.ZipEncodingHelper; import org.apache.commons.compress.utils.ArchiveUtils; import org.apache.commons.compress.utils.CharsetNames; import org.apache.commons.compress.utils.IOUtils; public class TarArchiveInputStream extends ArchiveInputStream { private static final int SMALL_BUFFER_SIZE = 256; private final byte[] SMALL_BUF = new byte[SMALL_BUFFER_SIZE];"
      },
      {
        "txt": "private final int recordSize; private final int blockSize; private boolean hasHitEOF; private long entrySize; private long entryOffset; private final InputStream is; private TarArchiveEntry currEntry; private final ZipEncoding zipEncoding; final String encoding; public TarArchiveInputStream(InputStream is) {"
      },
      {
        "txt": "this(is, TarConstants.DEFAULT_BLKSIZE, TarConstants.DEFAULT_RCDSIZE); } public TarArchiveInputStream(InputStream is, String encoding) { this(is, TarConstants.DEFAULT_BLKSIZE, TarConstants.DEFAULT_RCDSIZE, encoding); } public TarArchiveInputStream(InputStream is, int blockSize) { this(is, blockSize, TarConstants.DEFAULT_RCDSIZE); } public TarArchiveInputStream(InputStream is, int blockSize,"
      },
      {
        "txt": "String encoding) { this(is, blockSize, TarConstants.DEFAULT_RCDSIZE, encoding); } public TarArchiveInputStream(InputStream is, int blockSize, int recordSize) { this(is, blockSize, recordSize, null); } public TarArchiveInputStream(InputStream is, int blockSize, int recordSize, String encoding) { this.is = is; this.hasHitEOF = false;"
      },
      {
        "txt": "this.encoding = encoding; this.zipEncoding = ZipEncodingHelper.getZipEncoding(encoding); this.recordSize = recordSize; this.blockSize = blockSize; } @Override public void close() throws IOException { is.close(); } public int getRecordSize() {"
      },
      {
        "txt": "return recordSize; } @Override public int available() throws IOException { if (entrySize - entryOffset > Integer.MAX_VALUE) { return Integer.MAX_VALUE; } return (int) (entrySize - entryOffset); } @Override"
      },
      {
        "txt": "public long skip(final long n) throws IOException { if (n <= 0) { return 0; } final long available = entrySize - entryOffset; final long skipped = is.skip(Math.min(n, available)); count(skipped); entryOffset += skipped; return skipped; }"
      },
      {
        "txt": "@Override public boolean markSupported() { return false; } @Override public void mark(int markLimit) { } @Override public synchronized void reset() { }"
      },
      {
        "txt": "public TarArchiveEntry getNextTarEntry() throws IOException { if (hasHitEOF) { return null; } if (currEntry != null) { IOUtils.skip(this, Long.MAX_VALUE); skipRecordPadding(); } byte[] headerBuf = getRecord(); if (headerBuf == null) {"
      },
      {
        "txt": "currEntry = null; return null; } try { currEntry = new TarArchiveEntry(headerBuf, zipEncoding); } catch (IllegalArgumentException e) { IOException ioe = new IOException(\"Error detected parsing the header\"); ioe.initCause(e); throw ioe; }"
      },
      {
        "txt": "entryOffset = 0; entrySize = currEntry.getSize(); if (currEntry.isGNULongLinkEntry()) { byte[] longLinkData = getLongNameData(); if (longLinkData == null) { return null; } currEntry.setLinkName(zipEncoding.decode(longLinkData)); } if (currEntry.isGNULongNameEntry()) {"
      },
      {
        "txt": "byte[] longNameData = getLongNameData(); if (longNameData == null) { return null; } currEntry.setName(zipEncoding.decode(longNameData)); } if (currEntry.isPaxHeader()){ // Process Pax headers paxHeaders(); } if (currEntry.isGNUSparse()){ // Process sparse files"
      },
      {
        "txt": "readGNUSparse(); } entrySize = currEntry.getSize(); return currEntry; } private void skipRecordPadding() throws IOException { if (this.entrySize > 0 && this.entrySize % this.recordSize != 0) { long numRecords = (this.entrySize / this.recordSize) + 1; long padding = (numRecords * this.recordSize) - this.entrySize; long skipped = IOUtils.skip(is, padding);"
      },
      {
        "txt": "count(skipped); } } protected byte[] getLongNameData() throws IOException { ByteArrayOutputStream longName = new ByteArrayOutputStream(); int length = 0; while ((length = read(SMALL_BUF)) >= 0) { longName.write(SMALL_BUF, 0, length); } getNextEntry();"
      },
      {
        "txt": "if (currEntry == null) { return null; } byte[] longNameData = longName.toByteArray(); length = longNameData.length; while (length > 0 && longNameData[length - 1] == 0) { --length; } if (length != longNameData.length) { byte[] l = new byte[length];"
      },
      {
        "txt": "System.arraycopy(longNameData, 0, l, 0, length); longNameData = l; } return longNameData; } private byte[] getRecord() throws IOException { byte[] headerBuf = readRecord(); hasHitEOF = isEOFRecord(headerBuf); if (hasHitEOF && headerBuf != null) { tryToConsumeSecondEOFRecord();"
      },
      {
        "txt": "consumeRemainderOfLastBlock(); headerBuf = null; } return headerBuf; } protected boolean isEOFRecord(byte[] record) { return record == null || ArchiveUtils.isArrayZero(record, recordSize); } protected byte[] readRecord() throws IOException { byte[] record = new byte[recordSize];"
      },
      {
        "txt": "int readNow = IOUtils.readFully(is, record); count(readNow); if (readNow != recordSize) { return null; } return record; } private void paxHeaders() throws IOException{ Map<String, String> headers = parsePaxHeaders(this); getNextEntry(); // Get the actual file entry"
      },
      {
        "txt": "applyPaxHeadersToCurrentEntry(headers); } Map<String, String> parsePaxHeaders(InputStream i) throws IOException { Map<String, String> headers = new HashMap<String, String>(); while(true){ // get length int ch; int len = 0; int read = 0; while((ch = i.read()) != -1) { read++;"
      },
      {
        "txt": "if (ch == ' '){ // End of length string ByteArrayOutputStream coll = new ByteArrayOutputStream(); while((ch = i.read()) != -1) { read++; if (ch == '='){ // end of keyword String keyword = coll.toString(CharsetNames.UTF_8); final int restLen = len - read; byte[] rest = new byte[restLen]; int got = IOUtils.readFully(i, rest); if (got != restLen) {"
      },
      {
        "txt": "throw new IOException(\"Failed to read \" + \"Paxheader. Expected \" + restLen + \" bytes, read \" + got); } String value = new String(rest, 0, restLen - 1, CharsetNames.UTF_8); headers.put(keyword, value); break;"
      },
      {
        "txt": "} coll.write((byte) ch); } break; // Processed single header } len *= 10; len += ch - '0'; } if (ch == -1){ // EOF break;"
      },
      {
        "txt": "} } return headers; } private void applyPaxHeadersToCurrentEntry(Map<String, String> headers) { for (Entry<String, String> ent : headers.entrySet()){ String key = ent.getKey(); String val = ent.getValue(); if (\"path\".equals(key)){ currEntry.setName(val);"
      },
      {
        "txt": "currEntry.setLinkName(val); } else if (\"gid\".equals(key)){ currEntry.setGroupId(Integer.parseInt(val)); } else if (\"gname\".equals(key)){ currEntry.setGroupName(val); } else if (\"uid\".equals(key)){ <extra_id_0> } else if (\"uname\".equals(key)){ currEntry.setUserName(val); } else if (\"size\".equals(key)){ currEntry.setSize(Long.parseLong(val)); } else if (\"mtime\".equals(key)){ currEntry.setModTime((long) (Double.parseDouble(val) * 1000));"
      },
      {
        "txt": "} else if (\"mtime\".equals(key)){ currEntry.setModTime((long) (Double.parseDouble(val) * 1000)); } else if (\"SCHILY.devminor\".equals(key)){ currEntry.setDevMinor(Integer.parseInt(val)); } else if (\"SCHILY.devmajor\".equals(key)){ currEntry.setDevMajor(Integer.parseInt(val)); } } } private void readGNUSparse() throws IOException {"
      },
      {
        "txt": "sparses = new ArrayList(); sparses.addAll(currEntry.getSparses()); if (currEntry.isExtended()) { TarArchiveSparseEntry entry; do { byte[] headerBuf = getRecord(); if (headerBuf == null) { currEntry = null; break; }"
      },
      {
        "txt": "entry = new TarArchiveSparseEntry(headerBuf); sparses.addAll(entry.getSparses()); } while (entry.isExtended()); } } @Override public ArchiveEntry getNextEntry() throws IOException { return getNextTarEntry(); } private void tryToConsumeSecondEOFRecord() throws IOException {"
      },
      {
        "txt": "boolean shouldReset = true; boolean marked = is.markSupported(); if (marked) { is.mark(recordSize); } try { shouldReset = !isEOFRecord(readRecord()); } finally { if (shouldReset && marked) { pushedBackBytes(recordSize);"
      },
      {
        "txt": "is.reset(); } } } @Override public int read(byte[] buf, int offset, int numToRead) throws IOException { int totalRead = 0; if (hasHitEOF || entryOffset >= entrySize) { return -1; }"
      },
      {
        "txt": "if (currEntry == null) { throw new IllegalStateException(\"No current tar entry\"); } numToRead = Math.min(numToRead, available()); totalRead = is.read(buf, offset, numToRead); if (totalRead == -1) { if (numToRead > 0) { throw new IOException(\"Truncated TAR archive\"); } hasHitEOF = true;"
      },
      {
        "txt": "} else { count(totalRead); entryOffset += totalRead; } return totalRead; } @Override public boolean canReadEntryData(ArchiveEntry ae) { if (ae instanceof TarArchiveEntry) { TarArchiveEntry te = (TarArchiveEntry) ae;"
      },
      {
        "txt": "return !te.isGNUSparse(); } return false; } public TarArchiveEntry getCurrentEntry() { return currEntry; } protected final void setCurrentEntry(TarArchiveEntry e) { currEntry = e; }"
      },
      {
        "txt": "protected final boolean isAtEOF() { return hasHitEOF; } protected final void setAtEOF(boolean b) { hasHitEOF = b; } private void consumeRemainderOfLastBlock() throws IOException { long bytesReadOfLastBlock = getBytesRead() % blockSize; if (bytesReadOfLastBlock > 0) { long skipped = IOUtils.skip(is, blockSize - bytesReadOfLastBlock);"
      },
      {
        "txt": "count(skipped); } } public static boolean matches(byte[] signature, int length) { if (length < TarConstants.VERSION_OFFSET+TarConstants.VERSIONLEN) { return false; } if (ArchiveUtils.matchAsciiBuffer(TarConstants.MAGIC_POSIX, signature, TarConstants.MAGIC_OFFSET, TarConstants.MAGICLEN) &&"
      },
      {
        "txt": "ArchiveUtils.matchAsciiBuffer(TarConstants.VERSION_POSIX, signature, TarConstants.VERSION_OFFSET, TarConstants.VERSIONLEN) ){ return true; } if (ArchiveUtils.matchAsciiBuffer(TarConstants.MAGIC_GNU, signature, TarConstants.MAGIC_OFFSET, TarConstants.MAGICLEN) && ( ArchiveUtils.matchAsciiBuffer(TarConstants.VERSION_GNU_SPACE,"
      },
      {
        "txt": "signature, TarConstants.VERSION_OFFSET, TarConstants.VERSIONLEN) || ArchiveUtils.matchAsciiBuffer(TarConstants.VERSION_GNU_ZERO, signature, TarConstants.VERSION_OFFSET, TarConstants.VERSIONLEN) ) ){ return true; } if (ArchiveUtils.matchAsciiBuffer(TarConstants.MAGIC_ANT, signature, TarConstants.MAGIC_OFFSET, TarConstants.MAGICLEN)"
      },
      {
        "txt": "&& ArchiveUtils.matchAsciiBuffer(TarConstants.VERSION_ANT, signature, TarConstants.VERSION_OFFSET, TarConstants.VERSIONLEN) ){ return true; } return false; }"
      }
    ]
  }
]