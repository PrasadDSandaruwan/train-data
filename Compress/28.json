[
  {
    "id": 1066,
    "file_path": "src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveInputStream.java",
    "start-bug-line": 583,
    "end-bug-line": 583,
    "bug": "count(totalRead);",
    "fix": "[Delete]",
    "fixes": [],
    "err": "",
    "ctxs": [
      {
        "txt": "package org.apache.commons.compress.archivers.tar; import java.io.ByteArrayOutputStream; import java.io.IOException; import java.io.InputStream; import java.util.HashMap;"
      },
      {
        "txt": "import java.util.Map; import java.util.Map.Entry; import org.apache.commons.compress.archivers.ArchiveEntry; import org.apache.commons.compress.archivers.ArchiveInputStream; import org.apache.commons.compress.archivers.zip.ZipEncoding; import org.apache.commons.compress.archivers.zip.ZipEncodingHelper; import org.apache.commons.compress.utils.ArchiveUtils; import org.apache.commons.compress.utils.CharsetNames; import org.apache.commons.compress.utils.IOUtils; public class TarArchiveInputStream extends ArchiveInputStream {"
      },
      {
        "txt": "private static final int SMALL_BUFFER_SIZE = 256; private final byte[] SMALL_BUF = new byte[SMALL_BUFFER_SIZE]; private final int recordSize; private final int blockSize; private boolean hasHitEOF; private long entrySize; private long entryOffset; private final InputStream is; private TarArchiveEntry currEntry; private final ZipEncoding encoding;"
      },
      {
        "txt": "public TarArchiveInputStream(InputStream is) { this(is, TarConstants.DEFAULT_BLKSIZE, TarConstants.DEFAULT_RCDSIZE); } public TarArchiveInputStream(InputStream is, String encoding) { this(is, TarConstants.DEFAULT_BLKSIZE, TarConstants.DEFAULT_RCDSIZE, encoding); } public TarArchiveInputStream(InputStream is, int blockSize) { this(is, blockSize, TarConstants.DEFAULT_RCDSIZE); }"
      },
      {
        "txt": "public TarArchiveInputStream(InputStream is, int blockSize, String encoding) { this(is, blockSize, TarConstants.DEFAULT_RCDSIZE, encoding); } public TarArchiveInputStream(InputStream is, int blockSize, int recordSize) { this(is, blockSize, recordSize, null); } public TarArchiveInputStream(InputStream is, int blockSize, int recordSize, String encoding) { this.is = is;"
      },
      {
        "txt": "this.hasHitEOF = false; this.encoding = ZipEncodingHelper.getZipEncoding(encoding); this.recordSize = recordSize; this.blockSize = blockSize; } @Override public void close() throws IOException { is.close(); } public int getRecordSize() {"
      },
      {
        "txt": "return recordSize; } @Override public int available() throws IOException { if (entrySize - entryOffset > Integer.MAX_VALUE) { return Integer.MAX_VALUE; } return (int) (entrySize - entryOffset); } @Override"
      },
      {
        "txt": "public long skip(long numToSkip) throws IOException { long available = entrySize - entryOffset; numToSkip = Math.min(numToSkip, available); long skipped = IOUtils.skip(is, numToSkip); count(skipped); entryOffset += skipped; return skipped; } @Override public synchronized void reset() {"
      },
      {
        "txt": "} public TarArchiveEntry getNextTarEntry() throws IOException { if (hasHitEOF) { return null; } if (currEntry != null) { skip(Long.MAX_VALUE); skipRecordPadding(); } byte[] headerBuf = getRecord();"
      },
      {
        "txt": "if (headerBuf == null) { currEntry = null; return null; } try { currEntry = new TarArchiveEntry(headerBuf, encoding); } catch (IllegalArgumentException e) { IOException ioe = new IOException(\"Error detected parsing the header\"); ioe.initCause(e); throw ioe;"
      },
      {
        "txt": "} entryOffset = 0; entrySize = currEntry.getSize(); if (currEntry.isGNULongLinkEntry()) { byte[] longLinkData = getLongNameData(); if (longLinkData == null) { return null; } currEntry.setLinkName(encoding.decode(longLinkData)); }"
      },
      {
        "txt": "if (currEntry.isGNULongNameEntry()) { byte[] longNameData = getLongNameData(); if (longNameData == null) { return null; } currEntry.setName(encoding.decode(longNameData)); } if (currEntry.isPaxHeader()){ // Process Pax headers paxHeaders(); }"
      },
      {
        "txt": "if (currEntry.isGNUSparse()){ // Process sparse files readGNUSparse(); } entrySize = currEntry.getSize(); return currEntry; } private void skipRecordPadding() throws IOException { if (this.entrySize > 0 && this.entrySize % this.recordSize != 0) { long numRecords = (this.entrySize / this.recordSize) + 1; long padding = (numRecords * this.recordSize) - this.entrySize;"
      },
      {
        "txt": "long skipped = IOUtils.skip(is, padding); count(skipped); } } protected byte[] getLongNameData() throws IOException { ByteArrayOutputStream longName = new ByteArrayOutputStream(); int length = 0; while ((length = read(SMALL_BUF)) >= 0) { longName.write(SMALL_BUF, 0, length); }"
      },
      {
        "txt": "getNextEntry(); if (currEntry == null) { return null; } byte[] longNameData = longName.toByteArray(); length = longNameData.length; while (length > 0 && longNameData[length - 1] == 0) { --length; } if (length != longNameData.length) {"
      },
      {
        "txt": "byte[] l = new byte[length]; System.arraycopy(longNameData, 0, l, 0, length); longNameData = l; } return longNameData; } private byte[] getRecord() throws IOException { byte[] headerBuf = readRecord(); hasHitEOF = isEOFRecord(headerBuf); if (hasHitEOF && headerBuf != null) {"
      },
      {
        "txt": "tryToConsumeSecondEOFRecord(); consumeRemainderOfLastBlock(); headerBuf = null; } return headerBuf; } protected boolean isEOFRecord(byte[] record) { return record == null || ArchiveUtils.isArrayZero(record, recordSize); } protected byte[] readRecord() throws IOException {"
      },
      {
        "txt": "byte[] record = new byte[recordSize]; int readNow = IOUtils.readFully(is, record); count(readNow); if (readNow != recordSize) { return null; } return record; } private void paxHeaders() throws IOException{ Map<String, String> headers = parsePaxHeaders(this);"
      },
      {
        "txt": "getNextEntry(); // Get the actual file entry applyPaxHeadersToCurrentEntry(headers); } Map<String, String> parsePaxHeaders(InputStream i) throws IOException { Map<String, String> headers = new HashMap<String, String>(); while(true){ // get length int ch; int len = 0; int read = 0; while((ch = i.read()) != -1) {"
      },
      {
        "txt": "read++; if (ch == ' '){ // End of length string ByteArrayOutputStream coll = new ByteArrayOutputStream(); while((ch = i.read()) != -1) { read++; if (ch == '='){ // end of keyword String keyword = coll.toString(CharsetNames.UTF_8); byte[] rest = new byte[len - read]; int got = IOUtils.readFully(i, rest); if (got != len - read){"
      },
      {
        "txt": "throw new IOException(\"Failed to read \" + \"Paxheader. Expected \" + (len - read) + \" bytes, read \" + got); } String value = new String(rest, 0, len - read - 1, CharsetNames.UTF_8); headers.put(keyword, value); break;"
      },
      {
        "txt": "} coll.write((byte) ch); } break; // Processed single header } len *= 10; len += ch - '0'; } if (ch == -1){ // EOF break;"
      },
      {
        "txt": "} } return headers; } private void applyPaxHeadersToCurrentEntry(Map<String, String> headers) { for (Entry<String, String> ent : headers.entrySet()){ String key = ent.getKey(); String val = ent.getValue(); if (\"path\".equals(key)){ currEntry.setName(val);"
      },
      {
        "txt": "} else if (\"linkpath\".equals(key)){ currEntry.setLinkName(val); } else if (\"gid\".equals(key)){ currEntry.setGroupId(Integer.parseInt(val)); } else if (\"gname\".equals(key)){ currEntry.setGroupName(val); } else if (\"uid\".equals(key)){ currEntry.setUserId(Integer.parseInt(val)); } else if (\"uname\".equals(key)){ currEntry.setUserName(val);"
      },
      {
        "txt": "} else if (\"size\".equals(key)){ currEntry.setSize(Long.parseLong(val)); } else if (\"mtime\".equals(key)){ currEntry.setModTime((long) (Double.parseDouble(val) * 1000)); } else if (\"SCHILY.devminor\".equals(key)){ currEntry.setDevMinor(Integer.parseInt(val)); } else if (\"SCHILY.devmajor\".equals(key)){ currEntry.setDevMajor(Integer.parseInt(val)); } }"
      },
      {
        "txt": "} private void readGNUSparse() throws IOException { sparses = new ArrayList(); sparses.addAll(currEntry.getSparses()); if (currEntry.isExtended()) { TarArchiveSparseEntry entry; do { byte[] headerBuf = getRecord(); if (headerBuf == null) { currEntry = null;"
      },
      {
        "txt": "break; } entry = new TarArchiveSparseEntry(headerBuf); sparses.addAll(entry.getSparses()); } while (entry.isExtended()); } } @Override public ArchiveEntry getNextEntry() throws IOException { return getNextTarEntry();"
      },
      {
        "txt": "} private void tryToConsumeSecondEOFRecord() throws IOException { boolean shouldReset = true; boolean marked = is.markSupported(); if (marked) { is.mark(recordSize); } try { shouldReset = !isEOFRecord(readRecord()); } finally {"
      },
      {
        "txt": "if (shouldReset && marked) { pushedBackBytes(recordSize); is.reset(); } } } @Override public int read(byte[] buf, int offset, int numToRead) throws IOException { int totalRead = 0; if (hasHitEOF || entryOffset >= entrySize) {"
      },
      {
        "txt": "} if (currEntry == null) { throw new IllegalStateException(\"No current tar entry\"); } numToRead = Math.min(numToRead, available()); totalRead = is.read(buf, offset, numToRead); <extra_id_0> if (totalRead == -1) { hasHitEOF = true; } else { entryOffset += totalRead; } return totalRead;"
      },
      {
        "txt": "} return totalRead; } @Override public boolean canReadEntryData(ArchiveEntry ae) { if (ae instanceof TarArchiveEntry) { TarArchiveEntry te = (TarArchiveEntry) ae; return !te.isGNUSparse(); } return false;"
      },
      {
        "txt": "} public TarArchiveEntry getCurrentEntry() { return currEntry; } protected final void setCurrentEntry(TarArchiveEntry e) { currEntry = e; } protected final boolean isAtEOF() { return hasHitEOF; }"
      },
      {
        "txt": "protected final void setAtEOF(boolean b) { hasHitEOF = b; } private void consumeRemainderOfLastBlock() throws IOException { long bytesReadOfLastBlock = getBytesRead() % blockSize; if (bytesReadOfLastBlock > 0) { long skipped = IOUtils.skip(is, blockSize - bytesReadOfLastBlock); count(skipped); } }"
      },
      {
        "txt": "public static boolean matches(byte[] signature, int length) { if (length < TarConstants.VERSION_OFFSET+TarConstants.VERSIONLEN) { return false; } if (ArchiveUtils.matchAsciiBuffer(TarConstants.MAGIC_POSIX, signature, TarConstants.MAGIC_OFFSET, TarConstants.MAGICLEN) && ArchiveUtils.matchAsciiBuffer(TarConstants.VERSION_POSIX, signature, TarConstants.VERSION_OFFSET, TarConstants.VERSIONLEN) ){"
      },
      {
        "txt": "return true; } if (ArchiveUtils.matchAsciiBuffer(TarConstants.MAGIC_GNU, signature, TarConstants.MAGIC_OFFSET, TarConstants.MAGICLEN) && ( ArchiveUtils.matchAsciiBuffer(TarConstants.VERSION_GNU_SPACE, signature, TarConstants.VERSION_OFFSET, TarConstants.VERSIONLEN) || ArchiveUtils.matchAsciiBuffer(TarConstants.VERSION_GNU_ZERO,"
      },
      {
        "txt": "signature, TarConstants.VERSION_OFFSET, TarConstants.VERSIONLEN) ) ){ return true; } if (ArchiveUtils.matchAsciiBuffer(TarConstants.MAGIC_ANT, signature, TarConstants.MAGIC_OFFSET, TarConstants.MAGICLEN) && ArchiveUtils.matchAsciiBuffer(TarConstants.VERSION_ANT, signature, TarConstants.VERSION_OFFSET, TarConstants.VERSIONLEN)"
      },
      {
        "txt": "){ return true; } return false; }"
      }
    ]
  },
  {
    "id": 1067,
    "file_path": "src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveInputStream.java",
    "start-bug-line": 586,
    "end-bug-line": 586,
    "bug": "",
    "fix": "if (numToRead > 0) { throw new IOException(\"Truncated TAR archive\"); }",
    "fixes": [],
    "err": "",
    "ctxs": [
      {
        "txt": "package org.apache.commons.compress.archivers.tar; import java.io.ByteArrayOutputStream; import java.io.IOException; import java.io.InputStream; import java.util.HashMap; import java.util.Map; import java.util.Map.Entry;"
      },
      {
        "txt": "import org.apache.commons.compress.archivers.ArchiveEntry; import org.apache.commons.compress.archivers.ArchiveInputStream; import org.apache.commons.compress.archivers.zip.ZipEncoding; import org.apache.commons.compress.archivers.zip.ZipEncodingHelper; import org.apache.commons.compress.utils.ArchiveUtils; import org.apache.commons.compress.utils.CharsetNames; import org.apache.commons.compress.utils.IOUtils; public class TarArchiveInputStream extends ArchiveInputStream { private static final int SMALL_BUFFER_SIZE = 256; private final byte[] SMALL_BUF = new byte[SMALL_BUFFER_SIZE];"
      },
      {
        "txt": "private final int recordSize; private final int blockSize; private boolean hasHitEOF; private long entrySize; private long entryOffset; private final InputStream is; private TarArchiveEntry currEntry; private final ZipEncoding encoding; public TarArchiveInputStream(InputStream is) { this(is, TarConstants.DEFAULT_BLKSIZE, TarConstants.DEFAULT_RCDSIZE);"
      },
      {
        "txt": "} public TarArchiveInputStream(InputStream is, String encoding) { this(is, TarConstants.DEFAULT_BLKSIZE, TarConstants.DEFAULT_RCDSIZE, encoding); } public TarArchiveInputStream(InputStream is, int blockSize) { this(is, blockSize, TarConstants.DEFAULT_RCDSIZE); } public TarArchiveInputStream(InputStream is, int blockSize, String encoding) {"
      },
      {
        "txt": "this(is, blockSize, TarConstants.DEFAULT_RCDSIZE, encoding); } public TarArchiveInputStream(InputStream is, int blockSize, int recordSize) { this(is, blockSize, recordSize, null); } public TarArchiveInputStream(InputStream is, int blockSize, int recordSize, String encoding) { this.is = is; this.hasHitEOF = false; this.encoding = ZipEncodingHelper.getZipEncoding(encoding);"
      },
      {
        "txt": "this.recordSize = recordSize; this.blockSize = blockSize; } @Override public void close() throws IOException { is.close(); } public int getRecordSize() { return recordSize; }"
      },
      {
        "txt": "@Override public int available() throws IOException { if (entrySize - entryOffset > Integer.MAX_VALUE) { return Integer.MAX_VALUE; } return (int) (entrySize - entryOffset); } @Override public long skip(long numToSkip) throws IOException { long available = entrySize - entryOffset;"
      },
      {
        "txt": "numToSkip = Math.min(numToSkip, available); long skipped = IOUtils.skip(is, numToSkip); count(skipped); entryOffset += skipped; return skipped; } @Override public synchronized void reset() { } public TarArchiveEntry getNextTarEntry() throws IOException {"
      },
      {
        "txt": "if (hasHitEOF) { return null; } if (currEntry != null) { skip(Long.MAX_VALUE); skipRecordPadding(); } byte[] headerBuf = getRecord(); if (headerBuf == null) { currEntry = null;"
      },
      {
        "txt": "return null; } try { currEntry = new TarArchiveEntry(headerBuf, encoding); } catch (IllegalArgumentException e) { IOException ioe = new IOException(\"Error detected parsing the header\"); ioe.initCause(e); throw ioe; } entryOffset = 0;"
      },
      {
        "txt": "entrySize = currEntry.getSize(); if (currEntry.isGNULongLinkEntry()) { byte[] longLinkData = getLongNameData(); if (longLinkData == null) { return null; } currEntry.setLinkName(encoding.decode(longLinkData)); } if (currEntry.isGNULongNameEntry()) { byte[] longNameData = getLongNameData();"
      },
      {
        "txt": "if (longNameData == null) { return null; } currEntry.setName(encoding.decode(longNameData)); } if (currEntry.isPaxHeader()){ // Process Pax headers paxHeaders(); } if (currEntry.isGNUSparse()){ // Process sparse files readGNUSparse();"
      },
      {
        "txt": "} entrySize = currEntry.getSize(); return currEntry; } private void skipRecordPadding() throws IOException { if (this.entrySize > 0 && this.entrySize % this.recordSize != 0) { long numRecords = (this.entrySize / this.recordSize) + 1; long padding = (numRecords * this.recordSize) - this.entrySize; long skipped = IOUtils.skip(is, padding); count(skipped);"
      },
      {
        "txt": "} } protected byte[] getLongNameData() throws IOException { ByteArrayOutputStream longName = new ByteArrayOutputStream(); int length = 0; while ((length = read(SMALL_BUF)) >= 0) { longName.write(SMALL_BUF, 0, length); } getNextEntry(); if (currEntry == null) {"
      },
      {
        "txt": "return null; } byte[] longNameData = longName.toByteArray(); length = longNameData.length; while (length > 0 && longNameData[length - 1] == 0) { --length; } if (length != longNameData.length) { byte[] l = new byte[length]; System.arraycopy(longNameData, 0, l, 0, length);"
      },
      {
        "txt": "longNameData = l; } return longNameData; } private byte[] getRecord() throws IOException { byte[] headerBuf = readRecord(); hasHitEOF = isEOFRecord(headerBuf); if (hasHitEOF && headerBuf != null) { tryToConsumeSecondEOFRecord(); consumeRemainderOfLastBlock();"
      },
      {
        "txt": "headerBuf = null; } return headerBuf; } protected boolean isEOFRecord(byte[] record) { return record == null || ArchiveUtils.isArrayZero(record, recordSize); } protected byte[] readRecord() throws IOException { byte[] record = new byte[recordSize]; int readNow = IOUtils.readFully(is, record);"
      },
      {
        "txt": "count(readNow); if (readNow != recordSize) { return null; } return record; } private void paxHeaders() throws IOException{ Map<String, String> headers = parsePaxHeaders(this); getNextEntry(); // Get the actual file entry applyPaxHeadersToCurrentEntry(headers);"
      },
      {
        "txt": "} Map<String, String> parsePaxHeaders(InputStream i) throws IOException { Map<String, String> headers = new HashMap<String, String>(); while(true){ // get length int ch; int len = 0; int read = 0; while((ch = i.read()) != -1) { read++; if (ch == ' '){ // End of length string"
      },
      {
        "txt": "ByteArrayOutputStream coll = new ByteArrayOutputStream(); while((ch = i.read()) != -1) { read++; if (ch == '='){ // end of keyword String keyword = coll.toString(CharsetNames.UTF_8); byte[] rest = new byte[len - read]; int got = IOUtils.readFully(i, rest); if (got != len - read){ throw new IOException(\"Failed to read \" + \"Paxheader. Expected \""
      },
      {
        "txt": "+ (len - read) + \" bytes, read \" + got); } String value = new String(rest, 0, len - read - 1, CharsetNames.UTF_8); headers.put(keyword, value); break; } coll.write((byte) ch);"
      },
      {
        "txt": "} break; // Processed single header } len *= 10; len += ch - '0'; } if (ch == -1){ // EOF break; } }"
      },
      {
        "txt": "return headers; } private void applyPaxHeadersToCurrentEntry(Map<String, String> headers) { for (Entry<String, String> ent : headers.entrySet()){ String key = ent.getKey(); String val = ent.getValue(); if (\"path\".equals(key)){ currEntry.setName(val); } else if (\"linkpath\".equals(key)){ currEntry.setLinkName(val);"
      },
      {
        "txt": "} else if (\"gid\".equals(key)){ currEntry.setGroupId(Integer.parseInt(val)); } else if (\"gname\".equals(key)){ currEntry.setGroupName(val); } else if (\"uid\".equals(key)){ currEntry.setUserId(Integer.parseInt(val)); } else if (\"uname\".equals(key)){ currEntry.setUserName(val); } else if (\"size\".equals(key)){ currEntry.setSize(Long.parseLong(val));"
      },
      {
        "txt": "} else if (\"mtime\".equals(key)){ currEntry.setModTime((long) (Double.parseDouble(val) * 1000)); } else if (\"SCHILY.devminor\".equals(key)){ currEntry.setDevMinor(Integer.parseInt(val)); } else if (\"SCHILY.devmajor\".equals(key)){ currEntry.setDevMajor(Integer.parseInt(val)); } } } private void readGNUSparse() throws IOException {"
      },
      {
        "txt": "sparses = new ArrayList(); sparses.addAll(currEntry.getSparses()); if (currEntry.isExtended()) { TarArchiveSparseEntry entry; do { byte[] headerBuf = getRecord(); if (headerBuf == null) { currEntry = null; break; }"
      },
      {
        "txt": "entry = new TarArchiveSparseEntry(headerBuf); sparses.addAll(entry.getSparses()); } while (entry.isExtended()); } } @Override public ArchiveEntry getNextEntry() throws IOException { return getNextTarEntry(); } private void tryToConsumeSecondEOFRecord() throws IOException {"
      },
      {
        "txt": "boolean shouldReset = true; boolean marked = is.markSupported(); if (marked) { is.mark(recordSize); } try { shouldReset = !isEOFRecord(readRecord()); } finally { if (shouldReset && marked) { pushedBackBytes(recordSize);"
      },
      {
        "txt": "is.reset(); } } } @Override public int read(byte[] buf, int offset, int numToRead) throws IOException { int totalRead = 0; if (hasHitEOF || entryOffset >= entrySize) { return -1; }"
      },
      {
        "txt": "throw new IllegalStateException(\"No current tar entry\"); } numToRead = Math.min(numToRead, available()); totalRead = is.read(buf, offset, numToRead); count(totalRead); if (totalRead == -1) { <extra_id_0> } else { entryOffset += totalRead; } return totalRead; } @Override"
      },
      {
        "txt": "} @Override public boolean canReadEntryData(ArchiveEntry ae) { if (ae instanceof TarArchiveEntry) { TarArchiveEntry te = (TarArchiveEntry) ae; return !te.isGNUSparse(); } return false; } public TarArchiveEntry getCurrentEntry() {"
      },
      {
        "txt": "return currEntry; } protected final void setCurrentEntry(TarArchiveEntry e) { currEntry = e; } protected final boolean isAtEOF() { return hasHitEOF; } protected final void setAtEOF(boolean b) { hasHitEOF = b;"
      },
      {
        "txt": "} private void consumeRemainderOfLastBlock() throws IOException { long bytesReadOfLastBlock = getBytesRead() % blockSize; if (bytesReadOfLastBlock > 0) { long skipped = IOUtils.skip(is, blockSize - bytesReadOfLastBlock); count(skipped); } } public static boolean matches(byte[] signature, int length) { if (length < TarConstants.VERSION_OFFSET+TarConstants.VERSIONLEN) {"
      },
      {
        "txt": "return false; } if (ArchiveUtils.matchAsciiBuffer(TarConstants.MAGIC_POSIX, signature, TarConstants.MAGIC_OFFSET, TarConstants.MAGICLEN) && ArchiveUtils.matchAsciiBuffer(TarConstants.VERSION_POSIX, signature, TarConstants.VERSION_OFFSET, TarConstants.VERSIONLEN) ){ return true; }"
      },
      {
        "txt": "if (ArchiveUtils.matchAsciiBuffer(TarConstants.MAGIC_GNU, signature, TarConstants.MAGIC_OFFSET, TarConstants.MAGICLEN) && ( ArchiveUtils.matchAsciiBuffer(TarConstants.VERSION_GNU_SPACE, signature, TarConstants.VERSION_OFFSET, TarConstants.VERSIONLEN) || ArchiveUtils.matchAsciiBuffer(TarConstants.VERSION_GNU_ZERO, signature, TarConstants.VERSION_OFFSET, TarConstants.VERSIONLEN) )"
      },
      {
        "txt": "){ return true; } if (ArchiveUtils.matchAsciiBuffer(TarConstants.MAGIC_ANT, signature, TarConstants.MAGIC_OFFSET, TarConstants.MAGICLEN) && ArchiveUtils.matchAsciiBuffer(TarConstants.VERSION_ANT, signature, TarConstants.VERSION_OFFSET, TarConstants.VERSIONLEN) ){ return true;"
      },
      {
        "txt": "} return false; }"
      }
    ]
  },
  {
    "id": 1068,
    "file_path": "src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveInputStream.java",
    "start-bug-line": 588,
    "end-bug-line": 588,
    "bug": "",
    "fix": "count(totalRead);",
    "fixes": [],
    "err": "",
    "ctxs": [
      {
        "txt": "package org.apache.commons.compress.archivers.tar; import java.io.ByteArrayOutputStream; import java.io.IOException; import java.io.InputStream; import java.util.HashMap; import java.util.Map; import java.util.Map.Entry; import org.apache.commons.compress.archivers.ArchiveEntry; import org.apache.commons.compress.archivers.ArchiveInputStream;"
      },
      {
        "txt": "import org.apache.commons.compress.archivers.zip.ZipEncoding; import org.apache.commons.compress.archivers.zip.ZipEncodingHelper; import org.apache.commons.compress.utils.ArchiveUtils; import org.apache.commons.compress.utils.CharsetNames; import org.apache.commons.compress.utils.IOUtils; public class TarArchiveInputStream extends ArchiveInputStream { private static final int SMALL_BUFFER_SIZE = 256; private final byte[] SMALL_BUF = new byte[SMALL_BUFFER_SIZE]; private final int recordSize; private final int blockSize;"
      },
      {
        "txt": "private boolean hasHitEOF; private long entrySize; private long entryOffset; private final InputStream is; private TarArchiveEntry currEntry; private final ZipEncoding encoding; public TarArchiveInputStream(InputStream is) { this(is, TarConstants.DEFAULT_BLKSIZE, TarConstants.DEFAULT_RCDSIZE); } public TarArchiveInputStream(InputStream is, String encoding) {"
      },
      {
        "txt": "this(is, TarConstants.DEFAULT_BLKSIZE, TarConstants.DEFAULT_RCDSIZE, encoding); } public TarArchiveInputStream(InputStream is, int blockSize) { this(is, blockSize, TarConstants.DEFAULT_RCDSIZE); } public TarArchiveInputStream(InputStream is, int blockSize, String encoding) { this(is, blockSize, TarConstants.DEFAULT_RCDSIZE, encoding); }"
      },
      {
        "txt": "public TarArchiveInputStream(InputStream is, int blockSize, int recordSize) { this(is, blockSize, recordSize, null); } public TarArchiveInputStream(InputStream is, int blockSize, int recordSize, String encoding) { this.is = is; this.hasHitEOF = false; this.encoding = ZipEncodingHelper.getZipEncoding(encoding); this.recordSize = recordSize; this.blockSize = blockSize;"
      },
      {
        "txt": "} @Override public void close() throws IOException { is.close(); } public int getRecordSize() { return recordSize; } @Override public int available() throws IOException {"
      },
      {
        "txt": "if (entrySize - entryOffset > Integer.MAX_VALUE) { return Integer.MAX_VALUE; } return (int) (entrySize - entryOffset); } @Override public long skip(long numToSkip) throws IOException { long available = entrySize - entryOffset; numToSkip = Math.min(numToSkip, available); long skipped = IOUtils.skip(is, numToSkip);"
      },
      {
        "txt": "count(skipped); entryOffset += skipped; return skipped; } @Override public synchronized void reset() { } public TarArchiveEntry getNextTarEntry() throws IOException { if (hasHitEOF) { return null;"
      },
      {
        "txt": "} if (currEntry != null) { skip(Long.MAX_VALUE); skipRecordPadding(); } byte[] headerBuf = getRecord(); if (headerBuf == null) { currEntry = null; return null; }"
      },
      {
        "txt": "try { currEntry = new TarArchiveEntry(headerBuf, encoding); } catch (IllegalArgumentException e) { IOException ioe = new IOException(\"Error detected parsing the header\"); ioe.initCause(e); throw ioe; } entryOffset = 0; entrySize = currEntry.getSize(); if (currEntry.isGNULongLinkEntry()) {"
      },
      {
        "txt": "byte[] longLinkData = getLongNameData(); if (longLinkData == null) { return null; } currEntry.setLinkName(encoding.decode(longLinkData)); } if (currEntry.isGNULongNameEntry()) { byte[] longNameData = getLongNameData(); if (longNameData == null) { return null;"
      },
      {
        "txt": "} currEntry.setName(encoding.decode(longNameData)); } if (currEntry.isPaxHeader()){ // Process Pax headers paxHeaders(); } if (currEntry.isGNUSparse()){ // Process sparse files readGNUSparse(); } entrySize = currEntry.getSize();"
      },
      {
        "txt": "return currEntry; } private void skipRecordPadding() throws IOException { if (this.entrySize > 0 && this.entrySize % this.recordSize != 0) { long numRecords = (this.entrySize / this.recordSize) + 1; long padding = (numRecords * this.recordSize) - this.entrySize; long skipped = IOUtils.skip(is, padding); count(skipped); } }"
      },
      {
        "txt": "protected byte[] getLongNameData() throws IOException { ByteArrayOutputStream longName = new ByteArrayOutputStream(); int length = 0; while ((length = read(SMALL_BUF)) >= 0) { longName.write(SMALL_BUF, 0, length); } getNextEntry(); if (currEntry == null) { return null; }"
      },
      {
        "txt": "byte[] longNameData = longName.toByteArray(); length = longNameData.length; while (length > 0 && longNameData[length - 1] == 0) { --length; } if (length != longNameData.length) { byte[] l = new byte[length]; System.arraycopy(longNameData, 0, l, 0, length); longNameData = l; }"
      },
      {
        "txt": "return longNameData; } private byte[] getRecord() throws IOException { byte[] headerBuf = readRecord(); hasHitEOF = isEOFRecord(headerBuf); if (hasHitEOF && headerBuf != null) { tryToConsumeSecondEOFRecord(); consumeRemainderOfLastBlock(); headerBuf = null; }"
      },
      {
        "txt": "return headerBuf; } protected boolean isEOFRecord(byte[] record) { return record == null || ArchiveUtils.isArrayZero(record, recordSize); } protected byte[] readRecord() throws IOException { byte[] record = new byte[recordSize]; int readNow = IOUtils.readFully(is, record); count(readNow); if (readNow != recordSize) {"
      },
      {
        "txt": "return null; } return record; } private void paxHeaders() throws IOException{ Map<String, String> headers = parsePaxHeaders(this); getNextEntry(); // Get the actual file entry applyPaxHeadersToCurrentEntry(headers); } Map<String, String> parsePaxHeaders(InputStream i) throws IOException {"
      },
      {
        "txt": "Map<String, String> headers = new HashMap<String, String>(); while(true){ // get length int ch; int len = 0; int read = 0; while((ch = i.read()) != -1) { read++; if (ch == ' '){ // End of length string ByteArrayOutputStream coll = new ByteArrayOutputStream(); while((ch = i.read()) != -1) {"
      },
      {
        "txt": "read++; if (ch == '='){ // end of keyword String keyword = coll.toString(CharsetNames.UTF_8); byte[] rest = new byte[len - read]; int got = IOUtils.readFully(i, rest); if (got != len - read){ throw new IOException(\"Failed to read \" + \"Paxheader. Expected \" + (len - read) + \" bytes, read \""
      },
      {
        "txt": "+ got); } String value = new String(rest, 0, len - read - 1, CharsetNames.UTF_8); headers.put(keyword, value); break; } coll.write((byte) ch); } break; // Processed single header"
      },
      {
        "txt": "} len *= 10; len += ch - '0'; } if (ch == -1){ // EOF break; } } return headers; }"
      },
      {
        "txt": "private void applyPaxHeadersToCurrentEntry(Map<String, String> headers) { for (Entry<String, String> ent : headers.entrySet()){ String key = ent.getKey(); String val = ent.getValue(); if (\"path\".equals(key)){ currEntry.setName(val); } else if (\"linkpath\".equals(key)){ currEntry.setLinkName(val); } else if (\"gid\".equals(key)){ currEntry.setGroupId(Integer.parseInt(val));"
      },
      {
        "txt": "} else if (\"gname\".equals(key)){ currEntry.setGroupName(val); } else if (\"uid\".equals(key)){ currEntry.setUserId(Integer.parseInt(val)); } else if (\"uname\".equals(key)){ currEntry.setUserName(val); } else if (\"size\".equals(key)){ currEntry.setSize(Long.parseLong(val)); } else if (\"mtime\".equals(key)){ currEntry.setModTime((long) (Double.parseDouble(val) * 1000));"
      },
      {
        "txt": "} else if (\"SCHILY.devminor\".equals(key)){ currEntry.setDevMinor(Integer.parseInt(val)); } else if (\"SCHILY.devmajor\".equals(key)){ currEntry.setDevMajor(Integer.parseInt(val)); } } } private void readGNUSparse() throws IOException { sparses = new ArrayList(); sparses.addAll(currEntry.getSparses());"
      },
      {
        "txt": "if (currEntry.isExtended()) { TarArchiveSparseEntry entry; do { byte[] headerBuf = getRecord(); if (headerBuf == null) { currEntry = null; break; } entry = new TarArchiveSparseEntry(headerBuf); sparses.addAll(entry.getSparses());"
      },
      {
        "txt": "} while (entry.isExtended()); } } @Override public ArchiveEntry getNextEntry() throws IOException { return getNextTarEntry(); } private void tryToConsumeSecondEOFRecord() throws IOException { boolean shouldReset = true; boolean marked = is.markSupported();"
      },
      {
        "txt": "if (marked) { is.mark(recordSize); } try { shouldReset = !isEOFRecord(readRecord()); } finally { if (shouldReset && marked) { pushedBackBytes(recordSize); is.reset(); }"
      },
      {
        "txt": "} } @Override public int read(byte[] buf, int offset, int numToRead) throws IOException { int totalRead = 0; if (hasHitEOF || entryOffset >= entrySize) { return -1; } if (currEntry == null) { throw new IllegalStateException(\"No current tar entry\");"
      },
      {
        "txt": "numToRead = Math.min(numToRead, available()); totalRead = is.read(buf, offset, numToRead); count(totalRead); if (totalRead == -1) { hasHitEOF = true; } else { <extra_id_0> } return totalRead; } @Override public boolean canReadEntryData(ArchiveEntry ae) { if (ae instanceof TarArchiveEntry) {"
      },
      {
        "txt": "public boolean canReadEntryData(ArchiveEntry ae) { if (ae instanceof TarArchiveEntry) { TarArchiveEntry te = (TarArchiveEntry) ae; return !te.isGNUSparse(); } return false; } public TarArchiveEntry getCurrentEntry() { return currEntry; }"
      },
      {
        "txt": "protected final void setCurrentEntry(TarArchiveEntry e) { currEntry = e; } protected final boolean isAtEOF() { return hasHitEOF; } protected final void setAtEOF(boolean b) { hasHitEOF = b; } private void consumeRemainderOfLastBlock() throws IOException {"
      },
      {
        "txt": "long bytesReadOfLastBlock = getBytesRead() % blockSize; if (bytesReadOfLastBlock > 0) { long skipped = IOUtils.skip(is, blockSize - bytesReadOfLastBlock); count(skipped); } } public static boolean matches(byte[] signature, int length) { if (length < TarConstants.VERSION_OFFSET+TarConstants.VERSIONLEN) { return false; }"
      },
      {
        "txt": "if (ArchiveUtils.matchAsciiBuffer(TarConstants.MAGIC_POSIX, signature, TarConstants.MAGIC_OFFSET, TarConstants.MAGICLEN) && ArchiveUtils.matchAsciiBuffer(TarConstants.VERSION_POSIX, signature, TarConstants.VERSION_OFFSET, TarConstants.VERSIONLEN) ){ return true; } if (ArchiveUtils.matchAsciiBuffer(TarConstants.MAGIC_GNU, signature, TarConstants.MAGIC_OFFSET, TarConstants.MAGICLEN)"
      },
      {
        "txt": "&& ( ArchiveUtils.matchAsciiBuffer(TarConstants.VERSION_GNU_SPACE, signature, TarConstants.VERSION_OFFSET, TarConstants.VERSIONLEN) || ArchiveUtils.matchAsciiBuffer(TarConstants.VERSION_GNU_ZERO, signature, TarConstants.VERSION_OFFSET, TarConstants.VERSIONLEN) ) ){ return true;"
      },
      {
        "txt": "} if (ArchiveUtils.matchAsciiBuffer(TarConstants.MAGIC_ANT, signature, TarConstants.MAGIC_OFFSET, TarConstants.MAGICLEN) && ArchiveUtils.matchAsciiBuffer(TarConstants.VERSION_ANT, signature, TarConstants.VERSION_OFFSET, TarConstants.VERSIONLEN) ){ return true; } return false;"
      }
    ]
  }
]