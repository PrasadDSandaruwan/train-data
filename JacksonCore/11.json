[
  {
    "id": 1239,
    "file_path": "src/main/java/com/fasterxml/jackson/core/sym/ByteQuadsCanonicalizer.java",
    "start-bug-line": 882,
    "end-bug-line": 882,
    "bug": "",
    "fix": "_verifyNeedForRehash();",
    "fixes": [],
    "err": "",
    "ctxs": [
      {
        "txt": "package com.fasterxml.jackson.core.sym; import java.util.Arrays; import java.util.concurrent.atomic.AtomicReference; import com.fasterxml.jackson.core.JsonFactory; import com.fasterxml.jackson.core.util.InternCache; public final class ByteQuadsCanonicalizer { private static final int DEFAULT_T_SIZE = 64; private static final int MAX_T_SIZE = 0x10000; // 64k entries == 2M mem hash area final static int MIN_HASH_SIZE = 16;"
      },
      {
        "txt": "final static int MAX_ENTRIES_FOR_REUSE = 6000; final protected ByteQuadsCanonicalizer _parent; final protected AtomicReference<TableInfo> _tableInfo; final private int _seed; protected boolean _intern; protected final boolean _failOnDoS; protected int[] _hashArea; protected int _hashSize; protected int _secondaryStart; protected int _tertiaryStart;"
      },
      {
        "txt": "protected int _tertiaryShift; protected int _count; protected String[] _names; protected int _spilloverEnd; protected int _longNameOffset; private transient boolean _needRehash; private boolean _hashShared; private ByteQuadsCanonicalizer(int sz, boolean intern, int seed, boolean failOnDoS) { _parent = null; _seed = seed;"
      },
      {
        "txt": "_intern = intern; _failOnDoS = failOnDoS; if (sz < MIN_HASH_SIZE) { sz = MIN_HASH_SIZE; } else { if ((sz & (sz - 1)) != 0) { // only true if it's 2^N int curr = MIN_HASH_SIZE; while (curr < sz) { curr += curr; }"
      },
      {
        "txt": "sz = curr; } } _tableInfo = new AtomicReference<TableInfo>(TableInfo.createInitial(sz)); } private ByteQuadsCanonicalizer(ByteQuadsCanonicalizer parent, boolean intern, int seed, boolean failOnDoS, TableInfo state) { _parent = parent; _seed = seed;"
      },
      {
        "txt": "_intern = intern; _failOnDoS = failOnDoS; _tableInfo = null; // not used by child tables _count = state.count; _hashSize = state.size; _secondaryStart = _hashSize << 2; // right after primary area _tertiaryStart = _secondaryStart + (_secondaryStart >> 1); // right after secondary _tertiaryShift = state.tertiaryShift; _hashArea = state.mainHash; _names = state.names;"
      },
      {
        "txt": "_spilloverEnd = state.spilloverEnd; _longNameOffset = state.longNameOffset; _needRehash = false; _hashShared = true; } public static ByteQuadsCanonicalizer createRoot() { long now = System.currentTimeMillis(); int seed = (((int) now) + ((int) (now >>> 32))) | 1; return createRoot(seed); }"
      },
      {
        "txt": "protected static ByteQuadsCanonicalizer createRoot(int seed) { return new ByteQuadsCanonicalizer(DEFAULT_T_SIZE, true, seed, true); } public ByteQuadsCanonicalizer makeChild(int flags) { return new ByteQuadsCanonicalizer(this, JsonFactory.Feature.INTERN_FIELD_NAMES.enabledIn(flags), _seed, JsonFactory.Feature.FAIL_ON_SYMBOL_HASH_OVERFLOW.enabledIn(flags), _tableInfo.get()); }"
      },
      {
        "txt": "public void release() { if (_parent != null && maybeDirty()) { _parent.mergeChild(new TableInfo(this)); _hashShared = true; } } private void mergeChild(TableInfo childState) { final int childCount = childState.count;"
      },
      {
        "txt": "TableInfo currState = _tableInfo.get(); if (childCount == currState.count) { return; } if (childCount > MAX_ENTRIES_FOR_REUSE) { childState = TableInfo.createInitial(DEFAULT_T_SIZE); } _tableInfo.compareAndSet(currState, childState); } public int size()"
      },
      {
        "txt": "{ if (_tableInfo != null) { // root table return _tableInfo.get().count; } return _count; } public int bucketCount() { return _hashSize; } public boolean maybeDirty() { return !_hashShared; } public int hashSeed() { return _seed; } public int primaryCount()"
      },
      {
        "txt": "{ int count = 0; for (int offset = 3, end = _secondaryStart; offset < end; offset += 4) { if (_hashArea[offset] != 0) { ++count; } } return count; } public int secondaryCount() {"
      },
      {
        "txt": "int count = 0; int offset = _secondaryStart + 3; for (int end = _tertiaryStart; offset < end; offset += 4) { if (_hashArea[offset] != 0) { ++count; } } return count; } public int tertiaryCount() {"
      },
      {
        "txt": "int count = 0; int offset = _tertiaryStart + 3; // to 1.5x, starting point of tertiary for (int end = offset + _hashSize; offset < end; offset += 4) { if (_hashArea[offset] != 0) { ++count; } } return count; } public int spilloverCount() {"
      },
      {
        "txt": "return (_spilloverEnd - _spilloverStart()) >> 2; } public int totalCount() { int count = 0; for (int offset = 3, end = (_hashSize << 3); offset < end; offset += 4) { if (_hashArea[offset] != 0) { ++count; } }"
      },
      {
        "txt": "return count; } @Override public String toString() { int pri = primaryCount(); int sec = secondaryCount(); int tert = tertiaryCount(); int spill = spilloverCount(); int total = totalCount(); return String.format(\"[%s: size=%d, hashSize=%d, %d/%d/%d/%d pri/sec/ter/spill (=%s), total:%d]\","
      },
      {
        "txt": "getClass().getName(), _count, _hashSize, pri, sec, tert, spill, total, (pri+sec+tert+spill), total); } public String findName(int q1) { int offset = _calcOffset(calcHash(q1)); final int[] hashArea = _hashArea; int len = hashArea[offset+3]; if (len == 1) { if (hashArea[offset] == q1) {"
      },
      {
        "txt": "return _names[offset >> 2]; } } else if (len == 0) { // empty slot; unlikely but avoid further lookups if so return null; } int offset2 = _secondaryStart + ((offset >> 3) << 2); len = hashArea[offset2+3]; if (len == 1) { if (hashArea[offset2] == q1) { return _names[offset2 >> 2];"
      },
      {
        "txt": "} } else if (len == 0) { // empty slot; unlikely but avoid further lookups if so return null; } return _findSecondary(offset, q1); } public String findName(int q1, int q2) { int offset = _calcOffset(calcHash(q1, q2)); final int[] hashArea = _hashArea;"
      },
      {
        "txt": "int len = hashArea[offset+3]; if (len == 2) { if ((q1 == hashArea[offset]) && (q2 == hashArea[offset+1])) { return _names[offset >> 2]; } } else if (len == 0) { // empty slot; unlikely but avoid further lookups if so return null; } int offset2 = _secondaryStart + ((offset >> 3) << 2); len = hashArea[offset2+3];"
      },
      {
        "txt": "if (len == 2) { if ((q1 == hashArea[offset2]) && (q2 == hashArea[offset2+1])) { return _names[offset2 >> 2]; } } else if (len == 0) { // empty slot? Short-circuit if no more spillovers return null; } return _findSecondary(offset, q1, q2); } public String findName(int q1, int q2, int q3)"
      },
      {
        "txt": "{ int offset = _calcOffset(calcHash(q1, q2, q3)); final int[] hashArea = _hashArea; int len = hashArea[offset+3]; if (len == 3) { if ((q1 == hashArea[offset]) && (hashArea[offset+1] == q2) && (hashArea[offset+2] == q3)) { return _names[offset >> 2]; } } else if (len == 0) { // empty slot; unlikely but avoid further lookups if so return null;"
      },
      {
        "txt": "} int offset2 = _secondaryStart + ((offset >> 3) << 2); len = hashArea[offset2+3]; if (len == 3) { if ((q1 == hashArea[offset2]) && (hashArea[offset2+1] == q2) && (hashArea[offset2+2] == q3)) { return _names[offset2 >> 2]; } } else if (len == 0) { // empty slot? Short-circuit if no more spillovers return null; }"
      },
      {
        "txt": "return _findSecondary(offset, q1, q2, q3); } public String findName(int[] q, int qlen) { if (qlen < 4) { // another sanity check if (qlen == 3) { return findName(q[0], q[1], q[2]); } if (qlen == 2) { return findName(q[0], q[1]);"
      },
      {
        "txt": "} return findName(q[0]); } final int hash = calcHash(q, qlen); int offset = _calcOffset(hash); final int[] hashArea = _hashArea; final int len = hashArea[offset+3]; if ((hash == hashArea[offset]) && (len == qlen)) { if (_verifyLongName(q, qlen, hashArea[offset+1])) { return _names[offset >> 2];"
      },
      {
        "txt": "} } if (len == 0) { // empty slot; unlikely but avoid further lookups if so return null; } int offset2 = _secondaryStart + ((offset >> 3) << 2); final int len2 = hashArea[offset2+3]; if ((hash == hashArea[offset2]) && (len2 == qlen)) { if (_verifyLongName(q, qlen, hashArea[offset2+1])) { return _names[offset2 >> 2];"
      },
      {
        "txt": "} } if (len == 0) { // empty slot? Short-circuit if no more spillovers return null; } return _findSecondary(offset, hash, q, qlen); } private final int _calcOffset(int hash) { int ix = hash & (_hashSize-1);"
      },
      {
        "txt": "return (ix << 2); } private String _findSecondary(int origOffset, int q1) { int offset = _tertiaryStart + ((origOffset >> (_tertiaryShift + 2)) << _tertiaryShift); final int[] hashArea = _hashArea; final int bucketSize = (1 << _tertiaryShift); for (int end = offset + bucketSize; offset < end; offset += 4) { int len = hashArea[offset+3]; if ((q1 == hashArea[offset]) && (1 == len)) {"
      },
      {
        "txt": "return _names[offset >> 2]; } if (len == 0) { return null; } } for (offset = _spilloverStart(); offset < _spilloverEnd; offset += 4) { if ((q1 == hashArea[offset]) && (1 == hashArea[offset+3])) { return _names[offset >> 2]; }"
      },
      {
        "txt": "} return null; } private String _findSecondary(int origOffset, int q1, int q2) { int offset = _tertiaryStart + ((origOffset >> (_tertiaryShift + 2)) << _tertiaryShift); final int[] hashArea = _hashArea; final int bucketSize = (1 << _tertiaryShift); for (int end = offset + bucketSize; offset < end; offset += 4) { int len = hashArea[offset+3];"
      },
      {
        "txt": "if ((q1 == hashArea[offset]) && (q2 == hashArea[offset+1]) && (2 == len)) { return _names[offset >> 2]; } if (len == 0) { return null; } } for (offset = _spilloverStart(); offset < _spilloverEnd; offset += 4) { if ((q1 == hashArea[offset]) && (q2 == hashArea[offset+1]) && (2 == hashArea[offset+3])) { return _names[offset >> 2];"
      },
      {
        "txt": "} } return null; } private String _findSecondary(int origOffset, int q1, int q2, int q3) { int offset = _tertiaryStart + ((origOffset >> (_tertiaryShift + 2)) << _tertiaryShift); final int[] hashArea = _hashArea; final int bucketSize = (1 << _tertiaryShift); for (int end = offset + bucketSize; offset < end; offset += 4) {"
      },
      {
        "txt": "int len = hashArea[offset+3]; if ((q1 == hashArea[offset]) && (q2 == hashArea[offset+1]) && (q3 == hashArea[offset+2]) && (3 == len)) { return _names[offset >> 2]; } if (len == 0) { return null; } } for (offset = _spilloverStart(); offset < _spilloverEnd; offset += 4) { if ((q1 == hashArea[offset]) && (q2 == hashArea[offset+1]) && (q3 == hashArea[offset+2])"
      },
      {
        "txt": "&& (3 == hashArea[offset+3])) { return _names[offset >> 2]; } } return null; } private String _findSecondary(int origOffset, int hash, int[] q, int qlen) { int offset = _tertiaryStart + ((origOffset >> (_tertiaryShift + 2)) << _tertiaryShift); final int[] hashArea = _hashArea;"
      },
      {
        "txt": "final int bucketSize = (1 << _tertiaryShift); for (int end = offset + bucketSize; offset < end; offset += 4) { int len = hashArea[offset+3]; if ((hash == hashArea[offset]) && (qlen == len)) { if (_verifyLongName(q, qlen, hashArea[offset+1])) { return _names[offset >> 2]; } } if (len == 0) { return null;"
      },
      {
        "txt": "} } for (offset = _spilloverStart(); offset < _spilloverEnd; offset += 4) { if ((hash == hashArea[offset]) && (qlen == hashArea[offset+3])) { if (_verifyLongName(q, qlen, hashArea[offset+1])) { return _names[offset >> 2]; } } } return null;"
      },
      {
        "txt": "} private boolean _verifyLongName(int[] q, int qlen, int spillOffset) { final int[] hashArea = _hashArea; int ix = 0; switch (qlen) { default: return _verifyLongName2(q, qlen, spillOffset); case 8: if (q[ix++] != hashArea[spillOffset++]) return false;"
      },
      {
        "txt": "case 7: if (q[ix++] != hashArea[spillOffset++]) return false; case 6: if (q[ix++] != hashArea[spillOffset++]) return false; case 5: if (q[ix++] != hashArea[spillOffset++]) return false; case 4: // always at least 4 if (q[ix++] != hashArea[spillOffset++]) return false; if (q[ix++] != hashArea[spillOffset++]) return false; if (q[ix++] != hashArea[spillOffset++]) return false;"
      },
      {
        "txt": "if (q[ix++] != hashArea[spillOffset++]) return false; } return true; } private boolean _verifyLongName2(int[] q, int qlen, int spillOffset) { int ix = 0; do { if (q[ix++] != _hashArea[spillOffset++]) { return false;"
      },
      {
        "txt": "} } while (ix < qlen); return true; } public String addName(String name, int q1) { _verifySharing(); if (_intern) { name = InternCache.instance.intern(name); } int offset = _findOffsetForAdd(calcHash(q1));"
      },
      {
        "txt": "_hashArea[offset] = q1; _hashArea[offset+3] = 1; _names[offset >> 2] = name; ++_count; _verifyNeedForRehash(); return name; } public String addName(String name, int q1, int q2) { _verifySharing(); if (_intern) {"
      },
      {
        "txt": "name = InternCache.instance.intern(name); } int hash = (q2 == 0) ? calcHash(q1) : calcHash(q1, q2); int offset = _findOffsetForAdd(hash); _hashArea[offset] = q1; _hashArea[offset+1] = q2; _hashArea[offset+3] = 2; _names[offset >> 2] = name; ++_count; _verifyNeedForRehash();"
      },
      {
        "txt": "return name; } public String addName(String name, int q1, int q2, int q3) { _verifySharing(); if (_intern) { name = InternCache.instance.intern(name); } int offset = _findOffsetForAdd(calcHash(q1, q2, q3)); _hashArea[offset] = q1; _hashArea[offset+1] = q2;"
      },
      {
        "txt": "_hashArea[offset+2] = q3; _hashArea[offset+3] = 3; _names[offset >> 2] = name; ++_count; _verifyNeedForRehash(); return name; } public String addName(String name, int[] q, int qlen) { _verifySharing();"
      },
      {
        "txt": "if (_intern) { name = InternCache.instance.intern(name); } int offset; switch (qlen) { case 1: { offset = _findOffsetForAdd(calcHash(q[0])); _hashArea[offset] = q[0]; _hashArea[offset+3] = 1;"
      },
      {
        "txt": "} break; case 2: { offset = _findOffsetForAdd(calcHash(q[0], q[1])); _hashArea[offset] = q[0]; _hashArea[offset+1] = q[1]; _hashArea[offset+3] = 2; } break;"
      },
      {
        "txt": "case 3: { offset = _findOffsetForAdd(calcHash(q[0], q[1], q[2])); _hashArea[offset] = q[0]; _hashArea[offset+1] = q[1]; _hashArea[offset+2] = q[2]; _hashArea[offset+3] = 3; } break; default:"
      },
      {
        "txt": "final int hash = calcHash(q, qlen); offset = _findOffsetForAdd(hash); _hashArea[offset] = hash; int longStart = _appendLongName(q, qlen); _hashArea[offset+1] = longStart; _hashArea[offset+3] = qlen; } _names[offset >> 2] = name; ++_count; _verifyNeedForRehash();"
      },
      {
        "txt": "return name; } private void _verifyNeedForRehash() { if (_count > (_hashSize >> 1)) { // over 50% int spillCount = (_spilloverEnd - _spilloverStart()) >> 2; if ((spillCount > (1 + _count >> 7)) || (_count > (_hashSize * 0.80))) { _needRehash = true; } }"
      },
      {
        "txt": "private void _verifySharing() { if (_hashShared) { _hashArea = Arrays.copyOf(_hashArea, _hashArea.length); _names = Arrays.copyOf(_names, _names.length); _hashShared = false; <extra_id_0> if (_needRehash) { rehash(); } } private int _findOffsetForAdd(int hash) {"
      },
      {
        "txt": "private int _findOffsetForAdd(int hash) { int offset = _calcOffset(hash); final int[] hashArea = _hashArea; if (hashArea[offset+3] == 0) { return offset; } int offset2 = _secondaryStart + ((offset >> 3) << 2); if (hashArea[offset2+3] == 0) { return offset2;"
      },
      {
        "txt": "} offset2 = _tertiaryStart + ((offset >> (_tertiaryShift + 2)) << _tertiaryShift); final int bucketSize = (1 << _tertiaryShift); for (int end = offset2 + bucketSize; offset2 < end; offset2 += 4) { if (hashArea[offset2+3] == 0) { return offset2; } } offset = _spilloverEnd; _spilloverEnd += 4;"
      },
      {
        "txt": "final int end = (_hashSize << 3); if (_spilloverEnd >= end) { if (_failOnDoS) { _reportTooManyCollisions(); } _needRehash = true; } return offset; } private int _appendLongName(int[] quads, int qlen)"
      },
      {
        "txt": "{ int start = _longNameOffset; if ((start + qlen) > _hashArea.length) { int toAdd = (start + qlen) - _hashArea.length; int minAdd = Math.min(4096, _hashSize); int newSize = _hashArea.length + Math.max(toAdd, minAdd); _hashArea = Arrays.copyOf(_hashArea, newSize); } System.arraycopy(quads, 0, _hashArea, start, qlen); _longNameOffset += qlen;"
      },
      {
        "txt": "return start; } private final static int MULT = 33; private final static int MULT2 = 65599; private final static int MULT3 = 31; public int calcHash(int q1) { int hash = q1 ^ _seed; hash += (hash >>> 16); // to xor hi- and low- 16-bits hash ^= (hash << 3); // shuffle back a bit"
      },
      {
        "txt": "hash += (hash >>> 12); // and bit more return hash; } public int calcHash(int q1, int q2) { int hash = q1; hash += (hash >>> 15); // try mixing first and second byte pairs first hash ^= (hash >>> 9); // as well as lowest 2 bytes hash += (q2 * MULT); // then add second quad hash ^= _seed;"
      },
      {
        "txt": "hash += (hash >>> 16); // and shuffle some more hash ^= (hash >>> 4); hash += (hash << 3); return hash; } public int calcHash(int q1, int q2, int q3) { // use same algorithm as multi-byte, tested to work well int hash = q1 ^ _seed; hash += (hash >>> 9); hash *= MULT3;"
      },
      {
        "txt": "hash += q2; hash *= MULT; hash += (hash >>> 15); hash ^= q3; hash += (hash >>> 4); hash += (hash >>> 15); hash ^= (hash << 9); return hash; } public int calcHash(int[] q, int qlen)"
      },
      {
        "txt": "{ if (qlen < 4) { throw new IllegalArgumentException(); } int hash = q[0] ^ _seed; hash += (hash >>> 9); hash += q[1]; hash += (hash >>> 15); hash *= MULT; hash ^= q[2];"
      },
      {
        "txt": "hash += (hash >>> 4); for (int i = 3; i < qlen; ++i) { int next = q[i]; next = next ^ (next >> 21); hash += next; } hash *= MULT2; hash += (hash >>> 19); hash ^= (hash << 5); return hash;"
      },
      {
        "txt": "} private void rehash() { _needRehash = false; _hashShared = false; final int[] oldHashArea = _hashArea; final String[] oldNames = _names; final int oldSize = _hashSize; final int oldCount = _count; final int newSize = oldSize + oldSize;"
      },
      {
        "txt": "final int oldEnd = _spilloverEnd; if (newSize > MAX_T_SIZE) { nukeSymbols(true); return; } _hashArea = new int[oldHashArea.length + (oldSize<<3)]; _hashSize = newSize; _secondaryStart = (newSize << 2); // 4 ints per entry _tertiaryStart = _secondaryStart + (_secondaryStart >> 1); // right after secondary _tertiaryShift = _calcTertiaryShift(newSize);"
      },
      {
        "txt": "_names = new String[oldNames.length << 1]; nukeSymbols(false); int copyCount = 0; int[] q = new int[16]; for (int offset = 0, end = oldEnd; offset < end; offset += 4) { int len = oldHashArea[offset+3]; if (len == 0) { // empty slot, skip continue; } ++copyCount;"
      },
      {
        "txt": "String name = oldNames[offset>>2]; switch (len) { case 1: q[0] = oldHashArea[offset]; addName(name, q, 1); break; case 2: q[0] = oldHashArea[offset]; q[1] = oldHashArea[offset+1]; addName(name, q, 2);"
      },
      {
        "txt": "break; case 3: q[0] = oldHashArea[offset]; q[1] = oldHashArea[offset+1]; q[2] = oldHashArea[offset+2]; addName(name, q, 3); break; default: if (len > q.length) { q = new int[len];"
      },
      {
        "txt": "} int qoff = oldHashArea[offset+1]; System.arraycopy(oldHashArea, qoff, q, 0, len); addName(name, q, len); break; } } if (copyCount != oldCount) { throw new IllegalStateException(\"Failed rehash(): old count=\"+oldCount+\", copyCount=\"+copyCount); }"
      },
      {
        "txt": "} private void nukeSymbols(boolean fill) { _count = 0; _spilloverEnd = _spilloverStart(); _longNameOffset = _hashSize << 3; if (fill) { Arrays.fill(_hashArea, 0); Arrays.fill(_names, null); } }"
      },
      {
        "txt": "private final int _spilloverStart() { int offset = _hashSize; return (offset << 3) - offset; } protected void _reportTooManyCollisions() { if (_hashSize <= 1024) { // would have spill-over area of 128 entries return; } throw new IllegalStateException(\"Spill-over slots in symbol table with \"+_count"
      },
      {
        "txt": "+\" entries, hash area of \"+_hashSize+\" slots is now full (all \" +(_hashSize >> 3)+\" slots -- suspect a DoS attack based on hash collisions.\" +\" You can disable the check via `JsonFactory.Feature.FAIL_ON_SYMBOL_HASH_OVERFLOW`\"); } static int _calcTertiaryShift(int primarySlots) { int tertSlots = (primarySlots) >> 2; if (tertSlots < 64) { return 4; }"
      },
      {
        "txt": "if (tertSlots <= 256) { // buckets of 8 slots (up to 256 == 32 x 8) return 5; } if (tertSlots <= 1024) { // buckets of 16 slots (up to 1024 == 64 x 16) return 6; } return 7; } private final static class TableInfo {"
      },
      {
        "txt": "public final int size; public final int count; public final int tertiaryShift; public final int[] mainHash; public final String[] names; public final int spilloverEnd; public final int longNameOffset; public TableInfo(int size, int count, int tertiaryShift, int[] mainHash, String[] names, int spilloverEnd, int longNameOffset) {"
      },
      {
        "txt": "this.size = size; this.count = count; this.tertiaryShift = tertiaryShift; this.mainHash = mainHash; this.names = names; this.spilloverEnd = spilloverEnd; this.longNameOffset = longNameOffset; } public TableInfo(ByteQuadsCanonicalizer src) {"
      },
      {
        "txt": "size = src._hashSize; count = src._count; tertiaryShift = src._tertiaryShift; mainHash = src._hashArea; names = src._names; spilloverEnd = src._spilloverEnd; longNameOffset = src._longNameOffset; } public static TableInfo createInitial(int sz) { int hashAreaSize = sz << 3;"
      },
      {
        "txt": "int tertShift = _calcTertiaryShift(sz); return new TableInfo(sz, // hashSize 0, // count tertShift, new int[hashAreaSize], // mainHash, 2x slots, 4 ints per slot new String[sz << 1], // names == 2x slots hashAreaSize - sz, // at 7/8 of the total area hashAreaSize // longNameOffset, immediately after main hashes ); }"
      }
    ]
  }
]